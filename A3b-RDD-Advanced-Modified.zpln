{
  "paragraphs": [
    {
      "text": "%md\n# Big Data: Advanced Spark RDD",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:14+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763237_559023513",
      "id": "20210323-214017_1217973860",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:184",
      "dateFinished": "2021-04-13T13:25:14+0000",
      "dateStarted": "2021-04-13T13:25:14+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h1>Big Data: Advanced Spark RDD</h1>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "%md\n## Deeper Understanding of Spark\n\nThe goal of part B of Assignment 3 is deepen our understanding of what happens \"under the hood\".\n\nDo not just click shift enter on every cell, but try to grasp what is happening by looking into the Spark UI after executing each command.\n\nLearn the details by using variants of the example queries; create a new cell, or duplicate the cell and modify it. The knowledge you gain will be useful throughout the course, helpful in getting things to work at scale for the final assignment, and, help you pass the exam easily because you know how the design principles that are taught in the theory lectures work out in practice in actual code.\n\n### A Range of Numbers",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:14+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763238_500006090",
      "id": "20210323-214017_1348659670",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "status": "FINISHED",
      "$$hashKey": "object:185",
      "dateFinished": "2021-04-13T13:25:14+0000",
      "dateStarted": "2021-04-13T13:25:14+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Deeper Understanding of Spark</h2>\n<p>The goal of part B of Assignment 3 is deepen our understanding of what happens &ldquo;under the hood&rdquo;.</p>\n<p>Do not just click shift enter on every cell, but try to grasp what is happening by looking into the Spark UI after executing each command.</p>\n<p>Learn the details by using variants of the example queries; create a new cell, or duplicate the cell and modify it. The knowledge you gain will be useful throughout the course, helpful in getting things to work at scale for the final assignment, and, help you pass the exam easily because you know how the design principles that are taught in the theory lectures work out in practice in actual code.</p>\n<h3>A Range of Numbers</h3>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "%spark\nimport org.apache.spark.HashPartitioner\n\nval rddRange = sc.parallelize(0 to 999,8)",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:14+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763238_1773190226",
      "id": "20210323-214017_1526376678",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "dateStarted": "2021-04-13T13:25:14+0000",
      "dateFinished": "2021-04-13T13:25:15+0000",
      "status": "FINISHED",
      "$$hashKey": "object:186",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.HashPartitioner\n\u001b[1m\u001b[34mrddRange\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[Int]\u001b[0m = ParallelCollectionRDD[48] at parallelize at <console>:29\n"
          }
        ]
      }
    },
    {
      "text": "%md\n_That was quick!_\n\nRemember that the evaluation of Spark expressions is lazy, and only happens upon actions, not transformations; i.e., so far, nothing happened really happened, we only created an expression that specifies an RDD with 1000 values divided over 8 partitions.\n\nCheck the [Spark UI](http://localhost:4040): jobs and stages do not yet contain entries corresponding to this command.\n\n_Note: Port number `:4040` can be a higher one, e.g., `:4041`, depending on how many notebooks you run under the same kernel. If you did not see that coming, it may be better to restart the Docker container, or you'd have to add the extra port forwards to the `docker create` command._",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:15+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763238_623279341",
      "id": "20210323-214017_276769842",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "status": "FINISHED",
      "$$hashKey": "object:187",
      "dateFinished": "2021-04-13T13:25:15+0000",
      "dateStarted": "2021-04-13T13:25:15+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p><em>That was quick!</em></p>\n<p>Remember that the evaluation of Spark expressions is lazy, and only happens upon actions, not transformations; i.e., so far, nothing happened really happened, we only created an expression that specifies an RDD with 1000 values divided over 8 partitions.</p>\n<p>Check the <a href=\"http://localhost:4040\">Spark UI</a>: jobs and stages do not yet contain entries corresponding to this command.</p>\n<p><em>Note: Port number <code>:4040</code> can be a higher one, e.g., <code>:4041</code>, depending on how many notebooks you run under the same kernel. If you did not see that coming, it may be better to restart the Docker container, or you&rsquo;d have to add the extra port forwards to the <code>docker create</code> command.</em></p>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "%spark\nprintf( \"Number of partitions: %d\\n\", rddRange.partitions.length)\nrddRange.partitioner",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:15+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763238_1422687999",
      "id": "20210323-214017_1304985490",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "dateStarted": "2021-04-13T13:25:15+0000",
      "dateFinished": "2021-04-13T13:25:15+0000",
      "status": "FINISHED",
      "$$hashKey": "object:188",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Number of partitions: 8\n\u001b[1m\u001b[34mres89\u001b[0m: \u001b[1m\u001b[32mOption[org.apache.spark.Partitioner]\u001b[0m = None\n"
          }
        ]
      }
    },
    {
      "text": "%md\n_Partitions:_\n\nThe default number of partitions depends on the number of cores in the machine that runs the docker engine; see e.g., `cat /proc/cpuinfo` from a `bash` shell in the docker engine. In this case, we have taken control by specifying a desired number of partitions in the `parallelize` command. Spark does not know how the numbers are divided over the partitions, it does not understand that we used a Scala operation to create a sequence that consists of the first 1000 natural numbers.",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:15+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763239_1843908281",
      "id": "20210323-214017_1000819039",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "status": "FINISHED",
      "$$hashKey": "object:189",
      "dateFinished": "2021-04-13T13:25:15+0000",
      "dateStarted": "2021-04-13T13:25:15+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p><em>Partitions:</em></p>\n<p>The default number of partitions depends on the number of cores in the machine that runs the docker engine; see e.g., <code>cat /proc/cpuinfo</code> from a <code>bash</code> shell in the docker engine. In this case, we have taken control by specifying a desired number of partitions in the <code>parallelize</code> command. Spark does not know how the numbers are divided over the partitions, it does not understand that we used a Scala operation to create a sequence that consists of the first 1000 natural numbers.</p>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "%md\n## Pairs\nThe next command creates pairs of numbers, that we will treat as key-value pairs in the remainder of this notebook.\n\nIf you have not looked at Spark documentation so far, this would be a good time to go through the information on transformations and PairRDD functions:\n\n+ https://spark.apache.org/docs/3.1.1/rdd-programming-guide.html#transformations\n+ https://spark.apache.org/docs/3.1.1/api/scala/org/apache/spark/rdd/PairRDDFunctions.html\n\n_Right click on the link to open a new Tab in your browser (have you installed Firefox yet?) is easiest to process this information alongside the notebook itself._",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:15+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763239_1119108081",
      "id": "20210323-214017_1203666798",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "status": "FINISHED",
      "$$hashKey": "object:190",
      "dateFinished": "2021-04-13T13:25:15+0000",
      "dateStarted": "2021-04-13T13:25:15+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Pairs</h2>\n<p>The next command creates pairs of numbers, that we will treat as key-value pairs in the remainder of this notebook.</p>\n<p>If you have not looked at Spark documentation so far, this would be a good time to go through the information on transformations and PairRDD functions:</p>\n<ul>\n<li><a href=\"https://spark.apache.org/docs/3.1.1/rdd-programming-guide.html#transformations\">https://spark.apache.org/docs/3.1.1/rdd-programming-guide.html#transformations</a></li>\n<li><a href=\"https://spark.apache.org/docs/3.1.1/api/scala/org/apache/spark/rdd/PairRDDFunctions.html\">https://spark.apache.org/docs/3.1.1/api/scala/org/apache/spark/rdd/PairRDDFunctions.html</a></li>\n</ul>\n<p><em>Right click on the link to open a new Tab in your browser (have you installed Firefox yet?) is easiest to process this information alongside the notebook itself.</em></p>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "%spark\nval rddPairs = rddRange.map(x => (x % 100, 1000 - x))",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:26:57+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763239_1344183067",
      "id": "20210323-214017_753817829",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "dateStarted": "2021-04-13T13:26:57+0000",
      "dateFinished": "2021-04-13T13:26:58+0000",
      "status": "FINISHED",
      "$$hashKey": "object:191",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mrddPairs\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[(Int, Int)]\u001b[0m = MapPartitionsRDD[66] at map at <console>:29\n"
          }
        ]
      }
    },
    {
      "text": "%md\n_Think then act!_\n\nTry to understand what `rddPairs` consists of before executing the next cell. Can you draw on paper what the outcome of action `take(15)` will be when applied to `rddPairs`?",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:15+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763239_187083643",
      "id": "20210323-214017_575964677",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "status": "FINISHED",
      "$$hashKey": "object:192",
      "dateFinished": "2021-04-13T13:25:15+0000",
      "dateStarted": "2021-04-13T13:25:15+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p><em>Think then act!</em></p>\n<p>Try to understand what <code>rddPairs</code> consists of before executing the next cell. Can you draw on paper what the outcome of action <code>take(15)</code> will be when applied to <code>rddPairs</code>?</p>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "%spark\nrddPairs.take(500)",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:27:02+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://f681b3fcfe85:4040/jobs/job?id=119",
              "$$hashKey": "object:5670"
            },
            {
              "jobUrl": "http://f681b3fcfe85:4040/jobs/job?id=120",
              "$$hashKey": "object:5671"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763239_586967804",
      "id": "20210323-214017_599314081",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "dateStarted": "2021-04-13T13:27:02+0000",
      "dateFinished": "2021-04-13T13:27:03+0000",
      "status": "FINISHED",
      "$$hashKey": "object:193",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres114\u001b[0m: \u001b[1m\u001b[32mArray[(Int, Int)]\u001b[0m = Array((0,1000), (1,999), (2,998), (3,997), (4,996), (5,995), (6,994), (7,993), (8,992), (9,991), (10,990), (11,989), (12,988), (13,987), (14,986), (15,985), (16,984), (17,983), (18,982), (19,981), (20,980), (21,979), (22,978), (23,977), (24,976), (25,975), (26,974), (27,973), (28,972), (29,971), (30,970), (31,969), (32,968), (33,967), (34,966), (35,965), (36,964), (37,963), (38,962), (39,961), (40,960), (41,959), (42,958), (43,957), (44,956), (45,955), (46,954), (47,953), (48,952), (49,951), (50,950), (51,949), (52,948), (53,947), (54,946), (55,945), (56,944), (57,943), (58,942), (59,941), (60,940), (61,939), (62,938), (63,937), (64,936), (65,935), (66,934), (67,933), (68,932), (69,931), (70,930), (71,929), (72,928), (73,927), (74,92...\n"
          }
        ]
      }
    },
    {
      "text": "%spark\nrddPairs.mapPartitions(iter => Array(iter.size).iterator, true).collect()",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:16+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://f681b3fcfe85:4040/jobs/job?id=114",
              "$$hashKey": "object:4685"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618300792323_609578795",
      "id": "paragraph_1618300792323_609578795",
      "dateCreated": "2021-04-13T07:59:52+0000",
      "dateStarted": "2021-04-13T13:25:16+0000",
      "dateFinished": "2021-04-13T13:25:16+0000",
      "status": "FINISHED",
      "$$hashKey": "object:194",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres91\u001b[0m: \u001b[1m\u001b[32mArray[Int]\u001b[0m = Array(125, 125, 125, 125, 125, 125, 125, 125)\n"
          }
        ]
      }
    },
    {
      "text": "%spark\nrddPairs.partitioner",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:16+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763239_785482386",
      "id": "20210323-214017_399085751",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "dateStarted": "2021-04-13T13:25:16+0000",
      "dateFinished": "2021-04-13T13:25:16+0000",
      "status": "FINISHED",
      "$$hashKey": "object:195",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres92\u001b[0m: \u001b[1m\u001b[32mOption[org.apache.spark.Partitioner]\u001b[0m = None\n"
          }
        ]
      }
    },
    {
      "text": "%md\n_Partitions (continued):_\n\nAs we see, the default way of processing does not assign a partitioner; the framework partitions the data in the default way, which is merely a guess. You can however influence the way partitioning the RDD takes place, where the easiest one is to assign a partitioner manually.",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:16+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763239_1339632901",
      "id": "20210323-214017_1237244891",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "status": "FINISHED",
      "$$hashKey": "object:196",
      "dateFinished": "2021-04-13T13:25:16+0000",
      "dateStarted": "2021-04-13T13:25:16+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p><em>Partitions (continued):</em></p>\n<p>As we see, the default way of processing does not assign a partitioner; the framework partitions the data in the default way, which is merely a guess. You can however influence the way partitioning the RDD takes place, where the easiest one is to assign a partitioner manually.</p>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "%spark\nval rddPairsPart2 = rddPairs.partitionBy(new HashPartitioner(2))\n\nrddPairsPart2.mapPartitions(iter => Array(iter.size).iterator, true).collect()",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:16+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://f681b3fcfe85:4040/jobs/job?id=115",
              "$$hashKey": "object:4762"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763239_1324531639",
      "id": "20210323-214017_1339820103",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "dateStarted": "2021-04-13T13:25:16+0000",
      "dateFinished": "2021-04-13T13:25:17+0000",
      "status": "FINISHED",
      "$$hashKey": "object:197",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mrddPairsPart2\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[(Int, Int)]\u001b[0m = ShuffledRDD[51] at partitionBy at <console>:31\n\u001b[1m\u001b[34mres93\u001b[0m: \u001b[1m\u001b[32mArray[Int]\u001b[0m = Array(500, 500)\n"
          }
        ]
      }
    },
    {
      "text": "%md\n_Did you read the documentation so you can predict the outcome of the following cell?_",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:17+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763239_1692193063",
      "id": "20210323-214017_712252123",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "status": "FINISHED",
      "$$hashKey": "object:198",
      "dateFinished": "2021-04-13T13:25:17+0000",
      "dateStarted": "2021-04-13T13:25:17+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p><em>Did you read the documentation so you can predict the outcome of the following cell?</em></p>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "%spark\nprintf( \"Number of partitions: %d\\n\", rddPairsPart2.partitions.length)\nrddPairsPart2.partitioner",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:17+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763240_1890859644",
      "id": "20210323-214017_1910729482",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "dateStarted": "2021-04-13T13:25:17+0000",
      "dateFinished": "2021-04-13T13:25:17+0000",
      "status": "FINISHED",
      "$$hashKey": "object:199",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Number of partitions: 2\n\u001b[1m\u001b[34mres94\u001b[0m: \u001b[1m\u001b[32mOption[org.apache.spark.Partitioner]\u001b[0m = Some(org.apache.spark.HashPartitioner@2)\n"
          }
        ]
      }
    },
    {
      "text": "%md\n_Partitions and partitioners (continued):_\n\nSee how that influences processing; `rddPairs` is partitioned as we specified in `parallelize`, whereas `rddPairsPart2` consists of just two partitions, as a consequence of our instruction to `partitionBy` a `HashPartitioner`.",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:17+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763240_1658965729",
      "id": "20210323-214017_1320945390",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "status": "FINISHED",
      "$$hashKey": "object:200",
      "dateFinished": "2021-04-13T13:25:17+0000",
      "dateStarted": "2021-04-13T13:25:17+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p><em>Partitions and partitioners (continued):</em></p>\n<p>See how that influences processing; <code>rddPairs</code> is partitioned as we specified in <code>parallelize</code>, whereas <code>rddPairsPart2</code> consists of just two partitions, as a consequence of our instruction to <code>partitionBy</code> a <code>HashPartitioner</code>.</p>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "%spark\nval rddPairsGroup = rddPairs.groupByKey()\n\n\nrddPairsGroup.mapPartitions(iter => Array(iter.size).iterator, true).collect()",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:17+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://f681b3fcfe85:4040/jobs/job?id=116",
              "$$hashKey": "object:4851"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763240_331357896",
      "id": "20210323-214017_1628283214",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "dateStarted": "2021-04-13T13:25:17+0000",
      "dateFinished": "2021-04-13T13:25:17+0000",
      "status": "FINISHED",
      "$$hashKey": "object:201",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mrddPairsGroup\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[(Int, Iterable[Int])]\u001b[0m = ShuffledRDD[53] at groupByKey at <console>:31\n\u001b[1m\u001b[34mres95\u001b[0m: \u001b[1m\u001b[32mArray[Int]\u001b[0m = Array(13, 13, 13, 13, 12, 12, 12, 12)\n"
          }
        ]
      }
    },
    {
      "text": "%md\n_Look what happens!_\n\nWe will group the pairs by their keys -- which were created from the initial sequence by `x % 100`, i.e., there should be 10 different key values and therefore 10 different groups. Let's do that twice, and see the effect of partitioning on the (physical representation of the) result. Of course, logically, the results are identical -- in both cases, we are grouping the exact same dataset of pairs of numbers. However, physically, Spark has the freedom to organize data differently.\n\nFirst without a partitioner.",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:18+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763240_872203547",
      "id": "20210323-214017_1806028150",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "status": "FINISHED",
      "$$hashKey": "object:202",
      "dateFinished": "2021-04-13T13:25:18+0000",
      "dateStarted": "2021-04-13T13:25:18+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p><em>Look what happens!</em></p>\n<p>We will group the pairs by their keys &ndash; which were created from the initial sequence by <code>x % 100</code>, i.e., there should be 10 different key values and therefore 10 different groups. Let&rsquo;s do that twice, and see the effect of partitioning on the (physical representation of the) result. Of course, logically, the results are identical &ndash; in both cases, we are grouping the exact same dataset of pairs of numbers. However, physically, Spark has the freedom to organize data differently.</p>\n<p>First without a partitioner.</p>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "%spark\nrddPairsGroup.toDebugString",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:18+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763240_2131691145",
      "id": "20210323-214017_464846095",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "dateStarted": "2021-04-13T13:25:18+0000",
      "dateFinished": "2021-04-13T13:25:18+0000",
      "status": "FINISHED",
      "$$hashKey": "object:203",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres96\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m =\n(8) ShuffledRDD[53] at groupByKey at <console>:31 []\n +-(8) MapPartitionsRDD[49] at map at <console>:29 []\n    |  ParallelCollectionRDD[48] at parallelize at <console>:29 []\n"
          }
        ]
      }
    },
    {
      "text": "%md\n_Intermezzo:_\n\nThe data will be shuffled to create the groups. Spark does not know about the number of groups, even if we can figure it out ourselves. Efficient grouping operations over numeric data will use hashing, because the engine cannot know how many groups there are -- it only knows the cardinality and the number of partitions. After grouping, it can collect all the groups with the same hashcode at the seem \"reducer\" (using Map Reduce terminology).",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:18+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763240_366667097",
      "id": "20210323-214017_1727490445",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "status": "FINISHED",
      "$$hashKey": "object:204",
      "dateFinished": "2021-04-13T13:25:18+0000",
      "dateStarted": "2021-04-13T13:25:18+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p><em>Intermezzo:</em></p>\n<p>The data will be shuffled to create the groups. Spark does not know about the number of groups, even if we can figure it out ourselves. Efficient grouping operations over numeric data will use hashing, because the engine cannot know how many groups there are &ndash; it only knows the cardinality and the number of partitions. After grouping, it can collect all the groups with the same hashcode at the seem &ldquo;reducer&rdquo; (using Map Reduce terminology).</p>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "%spark\nprintf( \"Number of partitions: %d\\n\", rddPairsGroup.partitions.length)\nrddPairsGroup.partitioner",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:18+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763240_1266052135",
      "id": "20210323-214017_75408021",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "dateStarted": "2021-04-13T13:25:18+0000",
      "dateFinished": "2021-04-13T13:25:18+0000",
      "status": "FINISHED",
      "$$hashKey": "object:205",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Number of partitions: 8\n\u001b[1m\u001b[34mres97\u001b[0m: \u001b[1m\u001b[32mOption[org.apache.spark.Partitioner]\u001b[0m = Some(org.apache.spark.HashPartitioner@8)\n"
          }
        ]
      }
    },
    {
      "text": "%md\n_Grouping \"random\" ranges of numbers:_\n\nWe see that Spark _remembers_ the `HashPartitioner` it used to create the grouping. Because the original data had 8 partitions, it opts to create 8 partitions in the result as well, each with a different hashing function result.",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:18+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763240_1861523115",
      "id": "20210323-214017_721438644",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "status": "FINISHED",
      "$$hashKey": "object:206",
      "dateFinished": "2021-04-13T13:25:18+0000",
      "dateStarted": "2021-04-13T13:25:18+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p><em>Grouping &ldquo;random&rdquo; ranges of numbers:</em></p>\n<p>We see that Spark <em>remembers</em> the <code>HashPartitioner</code> it used to create the grouping. Because the original data had 8 partitions, it opts to create 8 partitions in the result as well, each with a different hashing function result.</p>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "%spark\nrddPairsGroup.partitions.map(p => (p, p.index, p.hashCode))",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:18+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763241_1257974358",
      "id": "20210323-214017_392207198",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "dateStarted": "2021-04-13T13:25:18+0000",
      "dateFinished": "2021-04-13T13:25:18+0000",
      "status": "FINISHED",
      "$$hashKey": "object:207",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres98\u001b[0m: \u001b[1m\u001b[32mArray[(org.apache.spark.Partition, Int, Int)]\u001b[0m = Array((org.apache.spark.rdd.ShuffledRDDPartition@0,0,0), (org.apache.spark.rdd.ShuffledRDDPartition@1,1,1), (org.apache.spark.rdd.ShuffledRDDPartition@2,2,2), (org.apache.spark.rdd.ShuffledRDDPartition@3,3,3), (org.apache.spark.rdd.ShuffledRDDPartition@4,4,4), (org.apache.spark.rdd.ShuffledRDDPartition@5,5,5), (org.apache.spark.rdd.ShuffledRDDPartition@6,6,6), (org.apache.spark.rdd.ShuffledRDDPartition@7,7,7))\n"
          }
        ]
      }
    },
    {
      "text": "%md\n_Grouping hashed ranges of numbers:_\n\nIn the other case, Spark has more knowledge about the ranges of values that exist in which partitions, and uses that prior information to choose a different physical plan to create the grouping.",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:19+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763241_1145247171",
      "id": "20210323-214017_1241916222",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "status": "FINISHED",
      "$$hashKey": "object:208",
      "dateFinished": "2021-04-13T13:25:19+0000",
      "dateStarted": "2021-04-13T13:25:19+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p><em>Grouping hashed ranges of numbers:</em></p>\n<p>In the other case, Spark has more knowledge about the ranges of values that exist in which partitions, and uses that prior information to choose a different physical plan to create the grouping.</p>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "%spark\nval rddPairsGroupPart2 = rddPairsPart2.groupByKey()\nrddPairsGroupPart2.partitioner",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:19+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763241_1452625995",
      "id": "20210323-214017_1000168161",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "dateStarted": "2021-04-13T13:25:19+0000",
      "dateFinished": "2021-04-13T13:25:19+0000",
      "status": "FINISHED",
      "$$hashKey": "object:209",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mrddPairsGroupPart2\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[(Int, Iterable[Int])]\u001b[0m = MapPartitionsRDD[55] at groupByKey at <console>:31\n\u001b[1m\u001b[34mres99\u001b[0m: \u001b[1m\u001b[32mOption[org.apache.spark.Partitioner]\u001b[0m = Some(org.apache.spark.HashPartitioner@2)\n"
          }
        ]
      }
    },
    {
      "text": "%spark\nrddPairsGroupPart2.partitions.map(p => (p, p.index, p.hashCode))",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:19+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763241_1216033858",
      "id": "20210323-214017_311003073",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "dateStarted": "2021-04-13T13:25:19+0000",
      "dateFinished": "2021-04-13T13:25:19+0000",
      "status": "FINISHED",
      "$$hashKey": "object:210",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres100\u001b[0m: \u001b[1m\u001b[32mArray[(org.apache.spark.Partition, Int, Int)]\u001b[0m = Array((org.apache.spark.rdd.ShuffledRDDPartition@0,0,0), (org.apache.spark.rdd.ShuffledRDDPartition@1,1,1))\n"
          }
        ]
      }
    },
    {
      "text": "%md\n_Intermezzo:_\n\nTake a close look at the differences between the results above, and form a mental model in line with the theory from the Spark thesis Chapter that we read for the course. Observe how the number of partitions of a groupByKey operation varies depending on the way the input is partitioned. This in turn affects the number of machines that will be at work in subsequent operations. Take a look at the Spark UI to understand how the different partitioning outcomes affect the number of mappers that will be used to compute the results.\n\n_Note: using explicit naming of RDDs helps you keep track of which job corresponds to which case._",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:19+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763241_614550108",
      "id": "20210323-214017_18896037",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "status": "FINISHED",
      "$$hashKey": "object:211",
      "dateFinished": "2021-04-13T13:25:19+0000",
      "dateStarted": "2021-04-13T13:25:19+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p><em>Intermezzo:</em></p>\n<p>Take a close look at the differences between the results above, and form a mental model in line with the theory from the Spark thesis Chapter that we read for the course. Observe how the number of partitions of a groupByKey operation varies depending on the way the input is partitioned. This in turn affects the number of machines that will be at work in subsequent operations. Take a look at the Spark UI to understand how the different partitioning outcomes affect the number of mappers that will be used to compute the results.</p>\n<p><em>Note: using explicit naming of RDDs helps you keep track of which job corresponds to which case.</em></p>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "%md\n_The story continues:_\n\nBelow, we expect you to figure out how the differences in processing come about. We no longer spell out the differences; even if so, do not just shift-enter through all the cells without thinking!\n\nConsider it a nice puzzle -- it works best if you try to predict what the outcome is, before you look at the actual results. Ideally, your blog post does not just copy these cryptically named results, but creates a story from a data problem where the decisions in partitioning make a difference and that difference can be explained to the reader. On ranges of a thousand numbers, it hardly matters what physical processing takes place, but when you analyze tens of Terabytes of Web crawl data, you better get the hang of Spark processing to succeed within reasonable execution times!",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:19+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763241_1088401254",
      "id": "20210323-214017_351811194",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "status": "FINISHED",
      "$$hashKey": "object:212",
      "dateFinished": "2021-04-13T13:25:19+0000",
      "dateStarted": "2021-04-13T13:25:19+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p><em>The story continues:</em></p>\n<p>Below, we expect you to figure out how the differences in processing come about. We no longer spell out the differences; even if so, do not just shift-enter through all the cells without thinking!</p>\n<p>Consider it a nice puzzle &ndash; it works best if you try to predict what the outcome is, before you look at the actual results. Ideally, your blog post does not just copy these cryptically named results, but creates a story from a data problem where the decisions in partitioning make a difference and that difference can be explained to the reader. On ranges of a thousand numbers, it hardly matters what physical processing takes place, but when you analyze tens of Terabytes of Web crawl data, you better get the hang of Spark processing to succeed within reasonable execution times!</p>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "%spark\nval rddPGP2Count = rddPairsGroupPart2.map( {case(x,y) => (x, y.reduce((a,b) => a + b))} )",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:19+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763241_8416790",
      "id": "20210323-214017_546452301",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "dateStarted": "2021-04-13T13:25:19+0000",
      "dateFinished": "2021-04-13T13:25:20+0000",
      "status": "FINISHED",
      "$$hashKey": "object:213",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mrddPGP2Count\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[(Int, Int)]\u001b[0m = MapPartitionsRDD[56] at map at <console>:29\n"
          }
        ]
      }
    },
    {
      "text": "%spark\nrddPGP2Count.name = \"Partitioned Group Counts (2)\"",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:20+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763241_1409898167",
      "id": "20210323-214017_1613266892",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "dateStarted": "2021-04-13T13:25:20+0000",
      "dateFinished": "2021-04-13T13:25:20+0000",
      "status": "FINISHED",
      "$$hashKey": "object:214",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "rddPGP2Count.name: String = Partitioned Group Counts (2)\n"
          }
        ]
      }
    },
    {
      "text": "%spark\nrddPGP2Count.take(10)",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:20+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://f681b3fcfe85:4040/jobs/job?id=117",
              "$$hashKey": "object:5138"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763241_1039020306",
      "id": "20210323-214017_1810635332",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "dateStarted": "2021-04-13T13:25:20+0000",
      "dateFinished": "2021-04-13T13:25:20+0000",
      "status": "FINISHED",
      "$$hashKey": "object:215",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres101\u001b[0m: \u001b[1m\u001b[32mArray[(Int, Int)]\u001b[0m = Array((34,5160), (52,4980), (96,4540), (4,5460), (16,5340), (82,4680), (66,4840), (28,5220), (54,4960), (80,4700))\n"
          }
        ]
      }
    },
    {
      "text": "%spark\nrddPGP2Count.partitions.size",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:20+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763242_276194760",
      "id": "20210323-214017_74392996",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "dateStarted": "2021-04-13T13:25:20+0000",
      "dateFinished": "2021-04-13T13:25:20+0000",
      "status": "FINISHED",
      "$$hashKey": "object:216",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres102\u001b[0m: \u001b[1m\u001b[32mInt\u001b[0m = 2\n"
          }
        ]
      }
    },
    {
      "text": "%spark\nrddPGP2Count.toDebugString",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:20+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763242_569260504",
      "id": "20210323-214017_894854719",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "dateStarted": "2021-04-13T13:25:20+0000",
      "dateFinished": "2021-04-13T13:25:21+0000",
      "status": "FINISHED",
      "$$hashKey": "object:217",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres103\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m =\n(2) Partitioned Group Counts (2) MapPartitionsRDD[56] at map at <console>:29 []\n |  MapPartitionsRDD[55] at groupByKey at <console>:31 []\n |  ShuffledRDD[51] at partitionBy at <console>:31 []\n +-(8) MapPartitionsRDD[49] at map at <console>:29 []\n    |  ParallelCollectionRDD[48] at parallelize at <console>:29 []\n"
          }
        ]
      }
    },
    {
      "text": "%spark\nprintf( \"Number of partitions: %d\\n\", rddPGP2Count.partitions.length)\nrddPGP2Count.partitioner",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:21+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763242_1129319733",
      "id": "20210323-214017_379828550",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "dateStarted": "2021-04-13T13:25:21+0000",
      "dateFinished": "2021-04-13T13:25:21+0000",
      "status": "FINISHED",
      "$$hashKey": "object:218",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Number of partitions: 2\n\u001b[1m\u001b[34mres104\u001b[0m: \u001b[1m\u001b[32mOption[org.apache.spark.Partitioner]\u001b[0m = None\n"
          }
        ]
      }
    },
    {
      "text": "%md\nQ: do you understand why the partitioner is none? \n\n_(No worries if not, you will find a clue below.)_",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:21+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763242_1981850151",
      "id": "20210323-214017_488026092",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "status": "FINISHED",
      "$$hashKey": "object:219",
      "dateFinished": "2021-04-13T13:25:21+0000",
      "dateStarted": "2021-04-13T13:25:21+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Q: do you understand why the partitioner is none?</p>\n<p><em>(No worries if not, you will find a clue below.)</em></p>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "%spark\nval rddPairsPart4 = rddPairs.partitionBy(new HashPartitioner(4))",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:21+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763242_1876457480",
      "id": "20210323-214017_1621748719",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "dateStarted": "2021-04-13T13:25:21+0000",
      "dateFinished": "2021-04-13T13:25:21+0000",
      "status": "FINISHED",
      "$$hashKey": "object:220",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mrddPairsPart4\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[(Int, Int)]\u001b[0m = ShuffledRDD[57] at partitionBy at <console>:29\n"
          }
        ]
      }
    },
    {
      "text": "%spark\nrddPairsPart4.take(10)",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:21+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://f681b3fcfe85:4040/jobs/job?id=118",
              "$$hashKey": "object:5290"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763242_172069511",
      "id": "20210323-214017_1724386907",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "dateStarted": "2021-04-13T13:25:21+0000",
      "dateFinished": "2021-04-13T13:25:22+0000",
      "status": "FINISHED",
      "$$hashKey": "object:221",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres105\u001b[0m: \u001b[1m\u001b[32mArray[(Int, Int)]\u001b[0m = Array((0,1000), (4,996), (8,992), (12,988), (16,984), (20,980), (24,976), (28,972), (32,968), (36,964))\n"
          }
        ]
      }
    },
    {
      "text": "%spark\nval rddA = rddPairsPart4.values.map( x => x  + 10 )",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:22+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763242_655157561",
      "id": "20210323-214017_1831531085",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "dateStarted": "2021-04-13T13:25:22+0000",
      "dateFinished": "2021-04-13T13:25:22+0000",
      "status": "FINISHED",
      "$$hashKey": "object:222",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mrddA\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[Int]\u001b[0m = MapPartitionsRDD[59] at map at <console>:29\n"
          }
        ]
      }
    },
    {
      "text": "%spark\nprintf( \"Number of partitions: %d\\n\", rddA.partitions.length)\nrddA.partitioner",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:22+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763242_1416856521",
      "id": "20210323-214017_1173051851",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "dateStarted": "2021-04-13T13:25:22+0000",
      "dateFinished": "2021-04-13T13:25:22+0000",
      "status": "FINISHED",
      "$$hashKey": "object:223",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Number of partitions: 4\n\u001b[1m\u001b[34mres106\u001b[0m: \u001b[1m\u001b[32mOption[org.apache.spark.Partitioner]\u001b[0m = None\n"
          }
        ]
      }
    },
    {
      "text": "%spark\nval rddB = rddPairsPart4.mapValues( x => x + 10 )",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:22+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763248_681279613",
      "id": "20210323-214017_1790059530",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "dateStarted": "2021-04-13T13:25:22+0000",
      "dateFinished": "2021-04-13T13:25:23+0000",
      "status": "FINISHED",
      "$$hashKey": "object:224",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mrddB\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[(Int, Int)]\u001b[0m = MapPartitionsRDD[60] at mapValues at <console>:29\n"
          }
        ]
      }
    },
    {
      "text": "%spark\nprintf( \"Number of partitions: %d\\n\", rddB.partitions.length)\nrddB.partitioner",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:23+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763249_90681926",
      "id": "20210323-214017_1286489553",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "dateStarted": "2021-04-13T13:25:23+0000",
      "dateFinished": "2021-04-13T13:25:23+0000",
      "status": "FINISHED",
      "$$hashKey": "object:225",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Number of partitions: 4\n\u001b[1m\u001b[34mres107\u001b[0m: \u001b[1m\u001b[32mOption[org.apache.spark.Partitioner]\u001b[0m = Some(org.apache.spark.HashPartitioner@4)\n"
          }
        ]
      }
    },
    {
      "text": "%md\n_Questions to let sink in:_\n\n+ Why are the results different for rddA and rddB? \n+ How is query processing affected by the partitioners?\n\n__Summarizing:__ partitioning depends on the distributed operations that are executed, and only operations with guarantees about the output distribution will carry an existing partitioner over to its result. The difference between `map` and `mapValues` has been discussed in the lectures, I hope you see the connection to the examples above!",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:23+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763249_1309505573",
      "id": "20210323-214017_1982627475",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "status": "FINISHED",
      "$$hashKey": "object:226",
      "dateFinished": "2021-04-13T13:25:23+0000",
      "dateStarted": "2021-04-13T13:25:23+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p><em>Questions to let sink in:</em></p>\n<ul>\n<li>Why are the results different for rddA and rddB?</li>\n<li>How is query processing affected by the partitioners?</li>\n</ul>\n<p><strong>Summarizing:</strong> partitioning depends on the distributed operations that are executed, and only operations with guarantees about the output distribution will carry an existing partitioner over to its result. The difference between <code>map</code> and <code>mapValues</code> has been discussed in the lectures, I hope you see the connection to the examples above!</p>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "%md\n_Controlling parallellism part 2:_\n\nAnother way to control the level of parallellism during query execution is to use the `repartition` and `coalesce` operation, to reorganize the data and thereby either increase the amount of parallellism (by creating more partitions) or reducing the overhead of too much parallellism (by merging existing partitions into a smaller number). Let's explore!",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:23+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763250_1210735036",
      "id": "20210323-214017_2128038071",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "status": "FINISHED",
      "$$hashKey": "object:227",
      "dateFinished": "2021-04-13T13:25:23+0000",
      "dateStarted": "2021-04-13T13:25:23+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p><em>Controlling parallellism part 2:</em></p>\n<p>Another way to control the level of parallellism during query execution is to use the <code>repartition</code> and <code>coalesce</code> operation, to reorganize the data and thereby either increase the amount of parallellism (by creating more partitions) or reducing the overhead of too much parallellism (by merging existing partitions into a smaller number). Let&rsquo;s explore!</p>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "%spark\nval rddC = rddA.repartition(2)\nrddC.partitions.size",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:23+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763250_324359640",
      "id": "20210323-214017_1632643197",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "dateStarted": "2021-04-13T13:25:23+0000",
      "dateFinished": "2021-04-13T13:25:23+0000",
      "status": "FINISHED",
      "$$hashKey": "object:228",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mrddC\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[Int]\u001b[0m = MapPartitionsRDD[64] at repartition at <console>:31\n\u001b[1m\u001b[34mres108\u001b[0m: \u001b[1m\u001b[32mInt\u001b[0m = 2\n"
          }
        ]
      }
    },
    {
      "text": "%spark\nrddC.partitions.map(p => (p, p.index, p.hashCode))",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:23+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763250_2045504566",
      "id": "20210323-214017_1546899510",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "dateStarted": "2021-04-13T13:25:23+0000",
      "dateFinished": "2021-04-13T13:25:24+0000",
      "status": "FINISHED",
      "$$hashKey": "object:229",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres109\u001b[0m: \u001b[1m\u001b[32mArray[(org.apache.spark.Partition, Int, Int)]\u001b[0m = Array((CoalescedRDDPartition(0,ShuffledRDD[62] at repartition at <console>:31,[I@5be8961,None),0,0), (CoalescedRDDPartition(1,ShuffledRDD[62] at repartition at <console>:31,[I@628a3cad,None),1,1))\n"
          }
        ]
      }
    },
    {
      "text": "%spark\nrddC.toDebugString",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:24+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763250_903247849",
      "id": "20210323-214017_1553511865",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "dateStarted": "2021-04-13T13:25:24+0000",
      "dateFinished": "2021-04-13T13:25:24+0000",
      "status": "FINISHED",
      "$$hashKey": "object:230",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres110\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m =\n(2) MapPartitionsRDD[64] at repartition at <console>:31 []\n |  CoalescedRDD[63] at repartition at <console>:31 []\n |  ShuffledRDD[62] at repartition at <console>:31 []\n +-(4) MapPartitionsRDD[61] at repartition at <console>:31 []\n    |  MapPartitionsRDD[59] at map at <console>:29 []\n    |  MapPartitionsRDD[58] at values at <console>:29 []\n    |  ShuffledRDD[57] at partitionBy at <console>:29 []\n    +-(8) MapPartitionsRDD[49] at map at <console>:29 []\n       |  ParallelCollectionRDD[48] at parallelize at <console>:29 []\n"
          }
        ]
      }
    },
    {
      "text": "%spark\nval rddD = rddB.coalesce(2)\nrddD.partitions.size",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:24+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763250_1733415335",
      "id": "20210323-214017_1102115685",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "dateStarted": "2021-04-13T13:25:24+0000",
      "dateFinished": "2021-04-13T13:25:24+0000",
      "status": "FINISHED",
      "$$hashKey": "object:231",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mrddD\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[(Int, Int)]\u001b[0m = CoalescedRDD[65] at coalesce at <console>:31\n\u001b[1m\u001b[34mres111\u001b[0m: \u001b[1m\u001b[32mInt\u001b[0m = 2\n"
          }
        ]
      }
    },
    {
      "text": "%spark\nrddD.toDebugString",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:24+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763250_201695239",
      "id": "20210323-214017_566438126",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "dateStarted": "2021-04-13T13:25:24+0000",
      "dateFinished": "2021-04-13T13:25:24+0000",
      "status": "FINISHED",
      "$$hashKey": "object:232",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres112\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m =\n(2) CoalescedRDD[65] at coalesce at <console>:31 []\n |  MapPartitionsRDD[60] at mapValues at <console>:29 []\n |  ShuffledRDD[57] at partitionBy at <console>:29 []\n +-(8) MapPartitionsRDD[49] at map at <console>:29 []\n    |  ParallelCollectionRDD[48] at parallelize at <console>:29 []\n"
          }
        ]
      }
    },
    {
      "text": "%md\n_No words, action!_\n\nRemember the lazy execution model -- only actions make things happen. Therefore, if you inspect the Spark UI right now, there are __no query plans__ that correspond to the above commands. Let us take two samples from the results.",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:25:24+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763251_1208842251",
      "id": "20210323-214017_1664351872",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "status": "FINISHED",
      "$$hashKey": "object:233",
      "dateFinished": "2021-04-13T13:25:24+0000",
      "dateStarted": "2021-04-13T13:25:24+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p><em>No words, action!</em></p>\n<p>Remember the lazy execution model &ndash; only actions make things happen. Therefore, if you inspect the Spark UI right now, there are <strong>no query plans</strong> that correspond to the above commands. Let us take two samples from the results.</p>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "%spark\nrddD.cache\nrddD.takeSample(true, 600);",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:53:06+0000",
      "progress": 42,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://f681b3fcfe85:4040/jobs/job?id=121",
              "$$hashKey": "object:5723"
            },
            {
              "jobUrl": "http://f681b3fcfe85:4040/jobs/job?id=122",
              "$$hashKey": "object:5724"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763251_1603086988",
      "id": "20210323-214017_1426084903",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "dateStarted": "2021-04-13T13:53:06+0000",
      "dateFinished": "2021-04-13T13:53:06+0000",
      "status": "FINISHED",
      "$$hashKey": "object:234",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres115\u001b[0m: \u001b[1m\u001b[32mArray[(Int, Int)]\u001b[0m = Array((47,763), (99,111), (48,762), (54,256), (27,183), (37,873), (48,362), (31,579), (4,806), (58,52), (21,289), (37,373), (38,472), (36,874), (53,757), (11,699), (68,742), (60,50), (71,939), (50,60), (48,362), (88,822), (19,691), (15,195), (65,545), (12,998), (18,792), (18,192), (88,522), (66,344), (90,320), (24,186), (21,589), (15,695), (14,696), (29,481), (25,85), (33,277), (38,272), (71,339), (66,844), (6,204), (40,670), (46,264), (13,697), (97,913), (17,293), (96,214), (47,263), (13,997), (93,517), (92,318), (17,693), (39,471), (25,885), (45,365), (94,616), (83,327), (75,935), (48,762), (88,322), (91,319), (52,658), (46,464), (60,150), (93,717), (98,512), (80,330), (79,731), (87,923), (66,444), (42,368), (9,801), (78,632), (43,...\n"
          }
        ]
      }
    },
    {
      "text": "%spark\nrddD.takeSample(true, 10);",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:53:13+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12,
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://f681b3fcfe85:4040/jobs/job?id=123",
              "$$hashKey": "object:5783"
            },
            {
              "jobUrl": "http://f681b3fcfe85:4040/jobs/job?id=124",
              "$$hashKey": "object:5784"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763251_301293201",
      "id": "20210323-214017_1541022546",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "dateStarted": "2021-04-13T13:53:13+0000",
      "dateFinished": "2021-04-13T13:53:13+0000",
      "status": "FINISHED",
      "$$hashKey": "object:235",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres116\u001b[0m: \u001b[1m\u001b[32mArray[(Int, Int)]\u001b[0m = Array((87,423), (22,788), (68,442), (2,508), (36,74), (2,208), (52,558), (61,449), (42,868), (3,607))\n"
          }
        ]
      }
    },
    {
      "text": "%md\nCan you find out how the jobs and stages came about, that you find [in the Spark UI](http://localhost:4040)?\n\nAlso, compare the two query plans for `rddC` and `rddD` that we printed in the cells above. Can you explain why the second query plan requires one shuffle phase less?",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:53:20+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763251_2032068539",
      "id": "20210323-214017_1099051495",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "status": "FINISHED",
      "$$hashKey": "object:236",
      "dateFinished": "2021-04-13T13:53:20+0000",
      "dateStarted": "2021-04-13T13:53:20+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Can you find out how the jobs and stages came about, that you find <a href=\"http://localhost:4040\">in the Spark UI</a>?</p>\n<p>Also, compare the two query plans for <code>rddC</code> and <code>rddD</code> that we printed in the cells above. Can you explain why the second query plan requires one shuffle phase less?</p>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "%md\n## Wrap up\n\nIf you have reached this point propery and understood what you observed, you have a solid understanding of Spark and its execution model.\n\nAssignment 4 will move up the stack to consider the Dataframe API.",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:53:25+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12,
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618149763251_370012265",
      "id": "20210323-214017_672918386",
      "dateCreated": "2021-04-11T14:02:43+0000",
      "status": "FINISHED",
      "$$hashKey": "object:237",
      "dateFinished": "2021-04-13T13:53:25+0000",
      "dateStarted": "2021-04-13T13:53:25+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Wrap up</h2>\n<p>If you have reached this point propery and understood what you observed, you have a solid understanding of Spark and its execution model.</p>\n<p>Assignment 4 will move up the stack to consider the Dataframe API.</p>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "%md\n",
      "user": "anonymous",
      "dateUpdated": "2021-04-13T13:53:25+0000",
      "progress": 0,
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1618322005060_28177446",
      "id": "paragraph_1618322005060_28177446",
      "dateCreated": "2021-04-13T13:53:25+0000",
      "status": "READY",
      "focus": true,
      "$$hashKey": "object:5826"
    }
  ],
  "name": "A3 Part B",
  "id": "2G5T9XXBF",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {},
  "path": "/A3 Part B"
}